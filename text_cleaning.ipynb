{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text using bnlp\n",
    "import re\n",
    "from bnlp import CleanText\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from phase 1\n",
    "dataset_path = \"data-source/bangla_sentiment.csv\"\n",
    "df = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "# Create the cleaner with your settings\n",
    "clean_text = CleanText(\n",
    "   fix_unicode=True,\n",
    "   unicode_norm=True,\n",
    "   unicode_norm_form=\"NFKC\",\n",
    "   remove_url=False,           # keep URLs as <URL>\n",
    "   replace_with_url=\"<URL>\",\n",
    "   remove_email=False,         # keep emails as <EMAIL>\n",
    "   replace_with_email=\"<EMAIL>\",\n",
    "   remove_emoji=True,        # keep emojis, or replace with <EMOJI>\n",
    "   remove_number=False,        # keep numbers\n",
    "   replace_with_number=\"<NUMBER>\",\n",
    "   remove_digits=False,        # keep ASCII digits\n",
    "   replace_with_digit=\"<DIGIT>\",\n",
    "   remove_punct=False,         # keep punctuation or replace\n",
    "   replace_with_punct=\"<PUNC>\"\n",
    ")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # BNLP cleaning\n",
    "    cleaned = clean_text(text)\n",
    "    # Additional regex for URLs/hashtags (if BNLP misses any)\n",
    "    cleaned = re.sub(r'http\\S+|#\\S+', '', cleaned)\n",
    "    # remove leading & trainling spaces\n",
    "    cleaned = cleaned.strip()\n",
    "    # Remove extra spaces\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    # Remove double quotes\n",
    "    cleaned = cleaned.replace('\"', '')\n",
    "    # Remove single quotes\n",
    "    cleaned = cleaned.replace(\"'\", '')\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# Apply cleaning on the 'Text' column (change 'Text' to your actual column name)\n",
    "df['Text'] = df['Text'].apply(preprocess_text)\n",
    "\n",
    "# Show the cleaned text\n",
    "#print(df[['Text', 'Cleaned_Text']].head())\n",
    "\n",
    "# Save the cleaned dataset\n",
    "df.to_csv(\"outputs/cleaned_dataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   Text  Label\n",
      "1     It s easy if they think কি হবে যদি আমরা রাসিয়...    0.0\n",
      "6                vai maybe science বানান টা ভুল হয়েছে।    0.0\n",
      "599   NASA জানিয়ে দিয়েছে মহাকাশ থেকে পৃথিবীর দিকে ...    0.0\n",
      "625   ওবায়দুল কাদেরের তৈলাক্ত ভার্সন হলো BPC Chairm...    0.0\n",
      "653   Joi Bangla সরকারের এমন উন্নয়নে সাধারণ মানুষের...    0.0\n",
      "691   তেলের দাম বেড়ে গেছে সস্তা জালালি তেল তৈরি Pla...    2.0\n",
      "969   বিশ্ববাজারে তেলের দাম বাড়ে নাই বরং বিশ্ব বাজা...    2.0\n",
      "1122  তেলের দাম বেড়ে গেছে সস্তা জালালি তেল তৈরি Pla...    2.0\n",
      "1163  সয়াবিন তেলের বদলে পানি use কর্সে বলে এখন ডিজে...    0.0\n",
      "1225      10tk তেলের দাম কমলে 20 দিন পরে 20 টাকা বাড়বে    0.0\n",
      "1235  তেলের দাম বেড়ে গেছে সস্তা জালালি তেল তৈরি Pla...    2.0\n",
      "1287  ২০ দিন আগে থেকেই কিনে খাচ্ছি ১৮০ টাকা করে So S...    2.0\n",
      "1649  তেলের দাম বেড়ে গেছে সস্তা জালালি তেল তৈরি Pla...    2.0\n",
      "1887  MF এর স্বর্ত পূরোনে জ্বালানি তেলের মূল্য বৃদ্ধ...    0.0\n",
      "2126                       50 taka বাড়িয়ে ৮ টাকা কমলো    0.0\n",
      "2383  ঠকবাজ UAE তে তেলের দাম বেশি সেখানে আমার ইনকামও...    0.0\n",
      "2384  আরে ভাই u a e এর কথা কেন বলতেচেন কাতারের কথা ব...    0.0\n",
      "2527  দেশে ডলারের দরকার হলে আমিরিকান প্রবাসী দের কাছ...    2.0\n",
      "2953  প্রাকৃতিক গ্যাস উড়ে যাচ্ছে আর সরকার ব্যাস্থ L...    0.0\n",
      "3069  IMF আসলে ভর্তুকি বাদ দিতে বলেনি তারা বলেছে তেল...    1.0\n",
      "3091  বর্তমান পরিস্থিতি ধামাচাপা দিতে সরকার BTS দের ...    2.0\n",
      "3438  Joi Bangla সরকারের এমন উন্নয়নে সাধারণ মানুষের...    1.0\n",
      "7775  সম্ভবত আমি এই লোকটা দেখছি খুব মার্জিত ভাষায় ক...    1.0\n",
      "8861    ফুডপাণ্ডা থেকে অর্ডার কেন্সেল হয়ে refund পাইনি    0.0\n",
      "8865                     Manikgonj delivery charge koto    2.0\n",
      "9088                         Order অনুযায়ী খাবার পাইনি    0.0\n"
     ]
    }
   ],
   "source": [
    "# Find rows where 'Text' contains at least one English letter\n",
    "has_english = df[df['Text'].str.contains(r'[a-zA-Z]', regex=True, na=False)]\n",
    "\n",
    "# Show these rows\n",
    "print(has_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_english.to_csv(\"outputs/english_words.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
