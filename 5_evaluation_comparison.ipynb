{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Evaluation and Comparison\n",
    "\n",
    "**Objective**: Compare baseline models (Phase 3) and mitigated models (Phase 4) to assess the effectiveness of imbalance mitigation strategies (SMOTE, Random Undersampling, NearMiss, Weighted Loss) for 3-class (Negative, Neutral, Positive) sentiment classification on the Bangla Sentiment Dataset. Evaluate performance on the test set, analyze source-specific performance (newspapers, social media, blogs) to test hypothesis H3 (source-specific differences in sentiment classification), and perform statistical tests to determine significant improvements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load Test Data and Models\n",
    "\n",
    "- **Objective**: Load test TF-IDF matrix, labels, source metadata, and all models (baseline and mitigated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:36:04,834 - INFO - Test data & Train Data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import joblib\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"text_representation/\"\n",
    "\n",
    "files = {\n",
    "    'tfidf_test': f\"{data_dir}tfidf_test.npz\",\n",
    "    'labels_test': f\"{data_dir}labels_test.csv\",\n",
    "    'tfidf_train': f\"{data_dir}tfidf_train.npz\",\n",
    "    'labels_train': f\"{data_dir}labels_train.csv\",\n",
    "}\n",
    "\n",
    "# Check file existence\n",
    "for name, path in files.items():\n",
    "    if not os.path.exists(path):\n",
    "        logging.error(f\"Missing file: {path}\")\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "# Load test data\n",
    "tfidf_test = sp.load_npz(files['tfidf_test'])\n",
    "y_test = pd.read_csv(files['labels_test'], encoding='utf-8')['Label'].values\n",
    "\n",
    "# load the train data for statistical testing\n",
    "# Placeholder: Replace with original training data\n",
    "X_full = sp.load_npz(files['tfidf_train'])\n",
    "y_full = pd.read_csv(files['labels_train'], encoding='utf-8')['Label'].values\n",
    "\n",
    "\n",
    "logging.info(\"Test data & Train Data loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading baseline models:   0%|          | 0/4 [00:00<?, ?it/s]/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-06-26 18:36:04,930 - INFO - Loaded model: baseline_LogisticRegression_tuned_grid\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-06-26 18:36:04,935 - INFO - Loaded model: baseline_SVM_tuned_grid\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-06-26 18:36:04,941 - INFO - Loaded model: baseline_NaiveBayes_tuned_grid\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-06-26 18:36:05,430 - INFO - Loaded model: baseline_RandomForest_tuned_grid\n",
      "Loading baseline models: 100%|██████████| 4/4 [00:00<00:00,  6.92it/s]\n",
      "Loading mitigated models:   0%|          | 0/16 [00:00<?, ?it/s]2025-06-26 18:36:05,436 - INFO - Loaded model: mitigated_LogisticRegression_smote_tuned\n",
      "2025-06-26 18:36:05,438 - INFO - Loaded model: mitigated_LogisticRegression_undersampled_tuned\n",
      "2025-06-26 18:36:05,440 - INFO - Loaded model: mitigated_LogisticRegression_nearmiss_tuned\n",
      "2025-06-26 18:36:05,443 - INFO - Loaded model: mitigated_LogisticRegression_weighted_tuned\n",
      "2025-06-26 18:36:05,449 - INFO - Loaded model: mitigated_SVM_smote_tuned\n",
      "2025-06-26 18:36:05,454 - INFO - Loaded model: mitigated_SVM_undersampled_tuned\n",
      "2025-06-26 18:36:05,459 - INFO - Loaded model: mitigated_SVM_nearmiss_tuned\n",
      "2025-06-26 18:36:05,466 - INFO - Loaded model: mitigated_SVM_weighted_tuned\n",
      "2025-06-26 18:36:05,468 - INFO - Loaded model: mitigated_NaiveBayes_smote_tuned\n",
      "2025-06-26 18:36:05,471 - INFO - Loaded model: mitigated_NaiveBayes_undersampled_tuned\n",
      "2025-06-26 18:36:05,475 - INFO - Loaded model: mitigated_NaiveBayes_nearmiss_tuned\n",
      "2025-06-26 18:36:05,479 - INFO - Loaded model: mitigated_NaiveBayes_weighted_tuned\n",
      "2025-06-26 18:36:05,870 - INFO - Loaded model: mitigated_RandomForest_smote_tuned\n",
      "Loading mitigated models:  81%|████████▏ | 13/16 [00:00<00:00, 29.77it/s]2025-06-26 18:36:05,960 - INFO - Loaded model: mitigated_RandomForest_undersampled_tuned\n",
      "2025-06-26 18:36:06,014 - INFO - Loaded model: mitigated_RandomForest_nearmiss_tuned\n",
      "2025-06-26 18:36:06,439 - INFO - Loaded model: mitigated_RandomForest_weighted_tuned\n",
      "Loading mitigated models: 100%|██████████| 16/16 [00:01<00:00, 15.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "model_dir_baseline = \"models/baseline_models/\"\n",
    "model_dir_mitigated = \"models/mitigated_models/\"\n",
    "\n",
    "model_configs = [\n",
    "    ('baseline', model_dir_baseline, ['LogisticRegression_tuned_grid', 'SVM_tuned_grid', 'NaiveBayes_tuned_grid', 'RandomForest_tuned_grid']),\n",
    "    ('mitigated', model_dir_mitigated, [\n",
    "        f\"{model}_{mitigation}_tuned\"\n",
    "        for model in ['LogisticRegression', 'SVM', 'NaiveBayes', 'RandomForest']\n",
    "        for mitigation in ['smote', 'undersampled', 'nearmiss', 'weighted']\n",
    "    ])\n",
    "]\n",
    "models = {}\n",
    "for config_type, model_dir, model_names in model_configs:\n",
    "    for name in tqdm(model_names, desc=f\"Loading {config_type} models\"):\n",
    "        try:\n",
    "            models[f\"{config_type}_{name}\"] = joblib.load(f\"{model_dir}{name}.joblib\")\n",
    "            logging.info(f\"Loaded model: {config_type}_{name}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error loading {name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Evaluate Models on Test Set\n",
    "\n",
    "- **Objective**: Compute accuracy, precision, recall, F1-score (weighted and per-class), and ROC-AUC for all models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating models:   0%|          | 0/20 [00:00<?, ?it/s]2025-06-26 18:36:06,472 - INFO - Evaluated: baseline_LogisticRegression_tuned_grid\n",
      "2025-06-26 18:36:07,455 - INFO - Evaluated: baseline_SVM_tuned_grid\n",
      "Evaluating models:  10%|█         | 2/20 [00:00<00:08,  2.01it/s]2025-06-26 18:36:07,465 - INFO - Evaluated: baseline_NaiveBayes_tuned_grid\n",
      "2025-06-26 18:36:07,691 - INFO - Evaluated: baseline_RandomForest_tuned_grid\n",
      "Evaluating models:  20%|██        | 4/20 [00:01<00:04,  3.64it/s]2025-06-26 18:36:07,699 - INFO - Evaluated: mitigated_LogisticRegression_smote_tuned\n",
      "2025-06-26 18:36:07,707 - INFO - Evaluated: mitigated_LogisticRegression_undersampled_tuned\n",
      "2025-06-26 18:36:07,715 - INFO - Evaluated: mitigated_LogisticRegression_nearmiss_tuned\n",
      "2025-06-26 18:36:07,724 - INFO - Evaluated: mitigated_LogisticRegression_weighted_tuned\n",
      "2025-06-26 18:36:08,700 - INFO - Evaluated: mitigated_SVM_smote_tuned\n",
      "Evaluating models:  45%|████▌     | 9/20 [00:02<00:02,  4.44it/s]2025-06-26 18:36:09,311 - INFO - Evaluated: mitigated_SVM_undersampled_tuned\n",
      "Evaluating models:  50%|█████     | 10/20 [00:02<00:02,  3.48it/s]2025-06-26 18:36:09,946 - INFO - Evaluated: mitigated_SVM_nearmiss_tuned\n",
      "Evaluating models:  55%|█████▌    | 11/20 [00:03<00:03,  2.83it/s]2025-06-26 18:36:10,981 - INFO - Evaluated: mitigated_SVM_weighted_tuned\n",
      "Evaluating models:  60%|██████    | 12/20 [00:04<00:03,  2.01it/s]2025-06-26 18:36:10,989 - INFO - Evaluated: mitigated_NaiveBayes_smote_tuned\n",
      "2025-06-26 18:36:10,996 - INFO - Evaluated: mitigated_NaiveBayes_undersampled_tuned\n",
      "2025-06-26 18:36:11,007 - INFO - Evaluated: mitigated_NaiveBayes_nearmiss_tuned\n",
      "2025-06-26 18:36:11,017 - INFO - Evaluated: mitigated_NaiveBayes_weighted_tuned\n",
      "2025-06-26 18:36:11,196 - INFO - Evaluated: mitigated_RandomForest_smote_tuned\n",
      "Evaluating models:  85%|████████▌ | 17/20 [00:04<00:00,  4.47it/s]2025-06-26 18:36:11,292 - INFO - Evaluated: mitigated_RandomForest_undersampled_tuned\n",
      "2025-06-26 18:36:11,379 - INFO - Evaluated: mitigated_RandomForest_nearmiss_tuned\n",
      "Evaluating models:  95%|█████████▌| 19/20 [00:04<00:00,  5.26it/s]2025-06-26 18:36:11,549 - INFO - Evaluated: mitigated_RandomForest_weighted_tuned\n",
      "Evaluating models: 100%|██████████| 20/20 [00:05<00:00,  3.93it/s]\n",
      "2025-06-26 18:36:11,555 - INFO - Saved comparative results to: evaluation/comparative_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    roc_auc_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"evaluation\", exist_ok=True)\n",
    "\n",
    "# Binarize true labels for multiclass ROC AUC\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "# Prepare results container\n",
    "results = []\n",
    "\n",
    "# Evaluate all models\n",
    "for model_key, model in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "    try:\n",
    "        # Parse model info\n",
    "        config_type, name = model_key.split('_', 1)\n",
    "        mitigation = name.split('_')[-1] if config_type == 'mitigated' else 'none'\n",
    "        model_name = '_'.join(name.split('_')[:-1]) if config_type == 'mitigated' else name.replace('_tuned_grid', '')\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(tfidf_test)\n",
    "        y_proba = model.predict_proba(tfidf_test)\n",
    "\n",
    "        # Evaluate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1_weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted')[2]\n",
    "        f1_per_class = precision_recall_fscore_support(y_test, y_pred, average=None)[2]\n",
    "        roc_auc = roc_auc_score(y_test_bin, y_proba, multi_class='ovr')\n",
    "\n",
    "        # Append results\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Type': config_type,\n",
    "            'Mitigation': mitigation,\n",
    "            'Accuracy': acc,\n",
    "            'F1_Weighted': f1_weighted,\n",
    "            'F1_Negative': f1_per_class[0],\n",
    "            'F1_Positive': f1_per_class[1],\n",
    "            'F1_Neutral': f1_per_class[2],\n",
    "            'ROC_AUC': roc_auc\n",
    "        })\n",
    "\n",
    "        logging.info(f\"Evaluated: {model_key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to evaluate {model_key}: {e}\")\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "csv_path = \"evaluation/comparative_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "logging.info(f\"Saved comparative results to: {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Results ===\n",
      "\n",
      "| Model                           | Type      | Mitigation   |   Accuracy |   F1_Weighted |   F1_Negative |   F1_Positive |   F1_Neutral |   ROC_AUC |\n",
      "|---------------------------------|-----------|--------------|------------|---------------|---------------|---------------|--------------|-----------|\n",
      "| LogisticRegression              | baseline  | none         |   0.629677 |      0.612208 |      0.715596 |      0.521739 |     0.51715  |  0.753958 |\n",
      "| SVM                             | baseline  | none         |   0.625806 |      0.612925 |      0.707692 |      0.530351 |     0.52551  |  0.734816 |\n",
      "| NaiveBayes                      | baseline  | none         |   0.621935 |      0.595792 |      0.710098 |      0.503597 |     0.48433  |  0.744348 |\n",
      "| RandomForest                    | baseline  | none         |   0.618065 |      0.593155 |      0.711602 |      0.478571 |     0.493151 |  0.743115 |\n",
      "| LogisticRegression_smote        | mitigated | tuned        |   0.574194 |      0.574455 |      0.648427 |      0.50411  |     0.511013 |  0.73087  |\n",
      "| LogisticRegression_undersampled | mitigated | tuned        |   0.593548 |      0.591833 |      0.656992 |      0.524064 |     0.54067  |  0.736189 |\n",
      "| LogisticRegression_nearmiss     | mitigated | tuned        |   0.56129  |      0.562888 |      0.612536 |      0.5      |     0.533058 |  0.720751 |\n",
      "| LogisticRegression_weighted     | mitigated | tuned        |   0.550968 |      0.553486 |      0.622159 |      0.480211 |     0.501071 |  0.707145 |\n",
      "| SVM_smote                       | mitigated | tuned        |   0.624516 |      0.610252 |      0.710588 |      0.520635 |     0.519481 |  0.731085 |\n",
      "| SVM_undersampled                | mitigated | tuned        |   0.621935 |      0.618953 |      0.690013 |      0.539945 |     0.567308 |  0.740134 |\n",
      "| SVM_nearmiss                    | mitigated | tuned        |   0.567742 |      0.568971 |      0.625    |      0.531073 |     0.508403 |  0.726973 |\n",
      "| SVM_weighted                    | mitigated | tuned        |   0.615484 |      0.602466 |      0.699052 |      0.518987 |     0.512821 |  0.730338 |\n",
      "| NaiveBayes_smote                | mitigated | tuned        |   0.549677 |      0.544981 |      0.631579 |      0.474777 |     0.460829 |  0.709405 |\n",
      "| NaiveBayes_undersampled         | mitigated | tuned        |   0.587097 |      0.586943 |      0.640434 |      0.523161 |     0.55157  |  0.739679 |\n",
      "| NaiveBayes_nearmiss             | mitigated | tuned        |   0.584516 |      0.58204  |      0.644068 |      0.52819  |     0.524664 |  0.726909 |\n",
      "| NaiveBayes_weighted             | mitigated | tuned        |   0.549677 |      0.544981 |      0.631579 |      0.474777 |     0.460829 |  0.709405 |\n",
      "| RandomForest_smote              | mitigated | tuned        |   0.596129 |      0.583424 |      0.684211 |      0.483871 |     0.5      |  0.731342 |\n",
      "| RandomForest_undersampled       | mitigated | tuned        |   0.616774 |      0.613338 |      0.684755 |      0.544444 |     0.552885 |  0.735062 |\n",
      "| RandomForest_nearmiss           | mitigated | tuned        |   0.552258 |      0.553687 |      0.596888 |      0.513228 |     0.516129 |  0.718303 |\n",
      "| RandomForest_weighted           | mitigated | tuned        |   0.596129 |      0.583424 |      0.684211 |      0.483871 |     0.5      |  0.731342 |\n"
     ]
    }
   ],
   "source": [
    "# Display as table in notebook output\n",
    "from tabulate import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "print(\"\\n=== Evaluation Results ===\\n\")\n",
    "print(tabulate(results_df, headers='keys', tablefmt='github', showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Statistical Tests\n",
    "\n",
    "- **Objective**: Perform paired t-tests to compare baseline vs. mitigated models’ F1-scores (weighted and Positive class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.base import clone\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(\"evaluation\", exist_ok=True)\n",
    "\n",
    "# Define setup\n",
    "rskf = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "metrics = {\n",
    "    'F1_Weighted': lambda y, y_pred: f1_score(y, y_pred, average='weighted'),\n",
    "    'F1_Positive': lambda y, y_pred: f1_score(y, y_pred, average=None)[1]\n",
    "}\n",
    "model_names = ['LogisticRegression', 'SVM', 'NaiveBayes', 'RandomForest']\n",
    "mitigations = ['smote', 'undersampled', 'nearmiss', 'weighted']\n",
    "\n",
    "# Results container\n",
    "stat_results = {\n",
    "    'Model': [],\n",
    "    'Mitigation': [],\n",
    "    'Metric': [],\n",
    "    'P_Value': [],\n",
    "    'Significant': []\n",
    "}\n",
    "\n",
    "# Perform statistical tests\n",
    "for model_name in model_names:\n",
    "    for mitigation in mitigations:\n",
    "        try:\n",
    "            baseline_key = f'baseline_{model_name}_tuned_grid'\n",
    "            mitigated_key = f'mitigated_{model_name}_{mitigation}_tuned'\n",
    "\n",
    "            base_model = models[baseline_key]\n",
    "            mitigated_model = models[mitigated_key]\n",
    "\n",
    "            # Store scores per metric\n",
    "            scores = {metric: {'base': [], 'mit': []} for metric in metrics}\n",
    "\n",
    "            for train_idx, test_idx in rskf.split(X_full, y_full):\n",
    "                X_train, X_test = X_full[train_idx], X_full[test_idx]\n",
    "                y_train, y_test = y_full[train_idx], y_full[test_idx]\n",
    "\n",
    "                # Train fresh clones of the models\n",
    "                base_clf = clone(base_model).fit(X_train, y_train)\n",
    "                mit_clf = clone(mitigated_model).fit(X_train, y_train)\n",
    "\n",
    "                y_pred_base = base_clf.predict(X_test)\n",
    "                y_pred_mit = mit_clf.predict(X_test)\n",
    "\n",
    "                for metric, func in metrics.items():\n",
    "                    scores[metric]['base'].append(func(y_test, y_pred_base))\n",
    "                    scores[metric]['mit'].append(func(y_test, y_pred_mit))\n",
    "\n",
    "            # Perform paired t-tests\n",
    "            for metric in metrics:\n",
    "                base_scores = scores[metric]['base']\n",
    "                mit_scores = scores[metric]['mit']\n",
    "                t_stat, p_value = ttest_rel(base_scores, mit_scores)\n",
    "\n",
    "                stat_results['Model'].append(model_name)\n",
    "                stat_results['Mitigation'].append(mitigation)\n",
    "                stat_results['Metric'].append(metric)\n",
    "                stat_results['P_Value'].append(p_value)\n",
    "                stat_results['Significant'].append(p_value < 0.05)\n",
    "\n",
    "                logging.info(f\"T-test | {model_name} | {mitigation} | {metric}: p={p_value:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {model_name} with {mitigation}: {str(e)}\")\n",
    "\n",
    "# Save results to CSV\n",
    "stat_results_df = pd.DataFrame(stat_results)\n",
    "output_path = \"evaluation/statistical_tests.csv\"\n",
    "stat_results_df.to_csv(output_path, index=False)\n",
    "logging.info(f\"Statistical test results saved: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stat_results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstat_results_df\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'stat_results_df' is not defined"
     ]
    }
   ],
   "source": [
    "stat_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
