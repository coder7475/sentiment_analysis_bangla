{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Preprocessing and Dataset Preparation\n",
    "\n",
    "**Objective**: Prepare the Bangla Sentiment Dataset (columns: Tense, Label) for model training, ensuring compatibility with traditional (TF-IDF-based) and neural (BanglaBERT) sentiment classification models while preserving class imbalance characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean Text (Remove Noise, Normalize Bangla Script)\n",
    "\n",
    "- **Objective**: Clean the `Tense` column to remove noise (e.g., special characters, URLs) and normalize Bangla text for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt not found. downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/fahad/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bnlp import CleanText\n",
    "    \n",
    "# Initialize BNLP cleaner\n",
    "clean_text = CleanText(\n",
    "    fix_unicode=True,\n",
    "    unicode_norm=True,\n",
    "    unicode_norm_form=\"NFKC\",\n",
    "    remove_url=False,\n",
    "    remove_email=False,\n",
    "    remove_emoji=False,\n",
    "    remove_number=False,\n",
    "    remove_digits=False,\n",
    "    remove_punct=False,\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"<DIGIT>\",\n",
    "    replace_with_punct = \"<PUNC>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tense</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tense  Label\n",
       "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...      0\n",
       "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...      1\n",
       "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from phase 1\n",
    "dataset_path = \"data-source/cleaned_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "class BanglaUnicodeNormalizer:\n",
    "    \"\"\"\n",
    "    Comprehensive Bangla text normalizer using Unicode standards.\n",
    "    Handles various inconsistencies while preserving linguistic accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._setup_unicode_mappings()\n",
    "        self._compile_patterns()\n",
    "    \n",
    "    def _setup_unicode_mappings(self):\n",
    "        \"\"\"Define Unicode character mappings for normalization.\"\"\"\n",
    "        \n",
    "        # Zero-width and invisible characters to remove/replace\n",
    "        self.invisible_chars = {\n",
    "            '\\u200d': '',      # Zero Width Joiner (ZWJ)\n",
    "            '\\u200c': '',      # Zero Width Non-Joiner (ZWNJ)\n",
    "            '\\u00a0': ' ',     # Non-breaking space → regular space\n",
    "            '\\ufeff': '',      # Byte Order Mark (BOM)\n",
    "            '\\u2060': '',      # Word Joiner\n",
    "            '\\u061c': '',      # Arabic Letter Mark\n",
    "        }\n",
    "        \n",
    "        # Bangla digit mappings (if normalization needed)\n",
    "        self.digit_mappings = {\n",
    "            '০': '0', '১': '1', '২': '2', '৩': '3', '৪': '4',\n",
    "            '৫': '5', '৬': '6', '৭': '7', '৮': '8', '৯': '9'\n",
    "        }\n",
    "        \n",
    "        # Punctuation normalization\n",
    "        self.punctuation_mappings = {\n",
    "            '॥': '।',         # Devanagari double danda → single danda\n",
    "            '‍': '',           # Zero width joiner variants\n",
    "            '‌': '',           # Zero width non-joiner variants\n",
    "            '।।': '।',        # Double danda → single\n",
    "        }\n",
    "        \n",
    "        # Vowel sign normalization (careful - these affect pronunciation)\n",
    "        self.vowel_normalizations = {\n",
    "            # Composite vowels that can be normalized\n",
    "            '\\u09c7\\u09be': '\\u09cb',  # ে + া = ো (e + aa = o)\n",
    "            '\\u09c7\\u09d7': '\\u09cc',  # ে + ৗ = ৌ (e + au-length = au)\n",
    "        }\n",
    "        \n",
    "        # Character variants that should be standardized\n",
    "        self.character_variants = {\n",
    "            # Only include mappings you're absolutely certain about\n",
    "            '\\u0995\\u09cd\\u09b7': '\\u0995\\u09cd\\u09b7',  # ক্ষ normalization\n",
    "        }\n",
    "        \n",
    "        # Common OCR/typing errors (use with caution)\n",
    "        self.ocr_corrections = {\n",
    "            # Add only well-established corrections\n",
    "            'ব়': 'ব',  # Remove nukta from ba if incorrectly added\n",
    "            'জ়': 'জ',  # Remove nukta from ja if incorrectly added\n",
    "        }\n",
    "    \n",
    "    def _compile_patterns(self):\n",
    "        \"\"\"Compile regex patterns for efficient processing.\"\"\"\n",
    "        \n",
    "        # Pattern for duplicate diacritics\n",
    "        self.duplicate_diacritics = re.compile(r'([ািীুূেৈোৌংঁঃ])\\1+')\n",
    "        \n",
    "        # Pattern for multiple whitespace\n",
    "        self.multiple_whitespace = re.compile(r'\\s+')\n",
    "        \n",
    "        # Pattern for Bangla digits\n",
    "        self.bangla_digits = re.compile(r'[০-৯]')\n",
    "        \n",
    "        # Pattern for ASCII digits\n",
    "        self.ascii_digits = re.compile(r'[0-9]')\n",
    "        \n",
    "        # Pattern for multiple punctuation\n",
    "        self.multiple_punct = re.compile(r'([।,;:!?])\\1+')\n",
    "        \n",
    "        # Pattern for hasanta (virama) normalization\n",
    "        self.hasanta_pattern = re.compile(r'\\u09cd(?=\\s|$)')\n",
    "    \n",
    "    def normalize_unicode(self, text: str, form: str = 'NFC') -> str:\n",
    "        \"\"\"\n",
    "        Apply Unicode normalization.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            form: Unicode normalization form ('NFC', 'NFD', 'NFKC', 'NFKD')\n",
    "        \n",
    "        Returns:\n",
    "            Unicode normalized text\n",
    "        \"\"\"\n",
    "        return unicodedata.normalize(form, text)\n",
    "    \n",
    "    def remove_invisible_chars(self, text: str) -> str:\n",
    "        pattern = re.compile('|'.join(map(re.escape, self.invisible_chars.keys())))\n",
    "        return pattern.sub(lambda m: self.invisible_chars[m.group(0)], text)\n",
    "\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normalize whitespace characters.\"\"\"\n",
    "        # Replace multiple whitespace with single space\n",
    "        text = self.multiple_whitespace.sub(' ', text)\n",
    "        # Strip leading/trailing whitespace\n",
    "        return text.strip()\n",
    "    \n",
    "    def normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Normalize punctuation marks.\"\"\"\n",
    "        for punct, normalized in self.punctuation_mappings.items():\n",
    "            text = text.replace(punct, normalized)\n",
    "        \n",
    "        # Handle multiple consecutive punctuation\n",
    "        text = self.multiple_punct.sub(r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def normalize_vowels(self, text: str) -> str:\n",
    "        \"\"\"Normalize vowel combinations and signs.\"\"\"\n",
    "        for combination, normalized in self.vowel_normalizations.items():\n",
    "            text = text.replace(combination, normalized)\n",
    "        \n",
    "        # Remove duplicate diacritics\n",
    "        text = self.duplicate_diacritics.sub(r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def normalize_digits(self, text: str, to_ascii: bool = False, remove: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Normalize digit representations.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            to_ascii: Convert Bangla digits to ASCII\n",
    "            remove: Remove all digits\n",
    "        \n",
    "        Returns:\n",
    "            Text with normalized digits\n",
    "        \"\"\"\n",
    "        if remove:\n",
    "            text = self.bangla_digits.sub('', text)\n",
    "            text = self.ascii_digits.sub('', text)\n",
    "        elif to_ascii:\n",
    "            for bangla, ascii_digit in self.digit_mappings.items():\n",
    "                text = text.replace(bangla, ascii_digit)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def apply_character_variants(self, text: str) -> str:\n",
    "        \"\"\"Apply character variant normalizations.\"\"\"\n",
    "        for variant, standard in self.character_variants.items():\n",
    "            text = text.replace(variant, standard)\n",
    "        return text\n",
    "    \n",
    "    def apply_ocr_corrections(self, text: str) -> str:\n",
    "        \"\"\"Apply common OCR error corrections (use with caution).\"\"\"\n",
    "        for error, correction in self.ocr_corrections.items():\n",
    "            text = text.replace(error, correction)\n",
    "        return text\n",
    "    \n",
    "    def normalize_hasanta(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize hasanta (virama) usage.\n",
    "        Remove trailing hasanta that don't form conjuncts.\n",
    "        \"\"\"\n",
    "        # Remove hasanta at word boundaries or end of text\n",
    "        text = self.hasanta_pattern.sub('', text)\n",
    "        return text\n",
    "    \n",
    "    def get_unicode_info(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get Unicode information for each character in text.\n",
    "        Useful for debugging normalization issues.\n",
    "        \"\"\"\n",
    "        info = []\n",
    "        for char in text:\n",
    "            info.append({\n",
    "                'char': char,\n",
    "                'unicode': f'U+{ord(char):04X}',\n",
    "                'name': unicodedata.name(char, 'UNKNOWN'),\n",
    "                'category': unicodedata.category(char),\n",
    "                'combining': unicodedata.combining(char)\n",
    "            })\n",
    "        return info\n",
    "    \n",
    "    def normalize(self, \n",
    "                 text: str, \n",
    "                 unicode_form: str = 'NFC',\n",
    "                 remove_digits: bool = False,\n",
    "                 digits_to_ascii: bool = False,\n",
    "                 apply_ocr_fixes: bool = False,\n",
    "                 normalize_hasanta: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Comprehensive text normalization.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to normalize\n",
    "            unicode_form: Unicode normalization form\n",
    "            remove_digits: Remove all digits\n",
    "            digits_to_ascii: Convert Bangla digits to ASCII\n",
    "            apply_ocr_fixes: Apply OCR error corrections\n",
    "            normalize_hasanta: Normalize hasanta usage\n",
    "        \n",
    "        Returns:\n",
    "            Normalized text\n",
    "        \"\"\"\n",
    "        \n",
    "        # Step 1: Unicode normalization\n",
    "        text = self.normalize_unicode(text, unicode_form)\n",
    "        \n",
    "        # Step 2: Remove invisible characters\n",
    "        text = self.remove_invisible_chars(text)\n",
    "        \n",
    "        # Step 3: Normalize whitespace\n",
    "        text = self.normalize_whitespace(text)\n",
    "        \n",
    "        # Step 4: Normalize punctuation\n",
    "        text = self.normalize_punctuation(text)\n",
    "        \n",
    "        # Step 5: Normalize vowels and diacritics\n",
    "        text = self.normalize_vowels(text)\n",
    "        \n",
    "        # Step 6: Handle digits\n",
    "        text = self.normalize_digits(text, digits_to_ascii, remove_digits)\n",
    "        \n",
    "        # Step 7: Apply character variants\n",
    "        text = self.apply_character_variants(text)\n",
    "        \n",
    "        # Step 8: Normalize hasanta (optional)\n",
    "        if normalize_hasanta:\n",
    "            text = self.normalize_hasanta(text)\n",
    "        \n",
    "        # Step 9: Apply OCR corrections (optional, use with caution)\n",
    "        if apply_ocr_fixes:\n",
    "            text = self.apply_ocr_corrections(text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "# Convenience function for quick normalization\n",
    "def normalize_bangla_text(text: str, **kwargs) -> str:\n",
    "    \"\"\"\n",
    "    Quick normalization function.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to normalize\n",
    "        **kwargs: Additional options for normalization\n",
    "    \n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \"\"\"\n",
    "    normalizer = BanglaUnicodeNormalizer()\n",
    "    return normalizer.normalize(text, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Cleaned Text:\n",
      "                                               Tense  \\\n",
      "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...   \n",
      "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...   \n",
      "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...   \n",
      "3          ওনার মতো ব্যর্থ মন্ত্রীর পদত্যাগ করা উচিত   \n",
      "4                 আল্লাহ তোদের বিচার করবে অপেক্ষা কর   \n",
      "\n",
      "                                       Tense_Cleaned  \n",
      "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...  \n",
      "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...  \n",
      "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...  \n",
      "3          ওনার মতো ব্যর্থ মন্ত্রীর পদত্যাগ করা উচিত  \n",
      "4                 আল্লাহ তোদের বিচার করবে অপেক্ষা কর  \n"
     ]
    }
   ],
   "source": [
    "# Initialize the normalizer once\n",
    "normalizer = BanglaUnicodeNormalizer()\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize Bangla text using BNLP and custom rules.\n",
    "    \"\"\"\n",
    "    # Step 1: BNLP text cleaning\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Step 2: Remove URLs and hashtags (if any missed)\n",
    "    cleaned = re.sub(r'(https?://\\S+|www\\.\\S+|#\\S+)', '', cleaned)\n",
    "    \n",
    "    # Step 3: Unicode normalization (NFC form)\n",
    "    cleaned = normalizer.normalize_unicode(cleaned, form='NFC')\n",
    "    \n",
    "    # Step 4: Remove invisible/control characters (ZWJ, ZWNJ, etc.)\n",
    "    cleaned = normalizer.remove_invisible_chars(cleaned)\n",
    "    \n",
    "    # Step 5: Normalize punctuation, whitespace, vowels, and hasanta\n",
    "    cleaned = normalizer.normalize_punctuation(cleaned)\n",
    "    cleaned = normalizer.normalize_vowels(cleaned)\n",
    "    cleaned = normalizer.normalize_whitespace(cleaned)\n",
    "    \n",
    "    # Step 6 (Optional): Remove digits or apply OCR correction if needed\n",
    "    cleaned = normalizer.normalize_digits(cleaned, remove=True)\n",
    "    # cleaned = normalizer.apply_ocr_corrections(cleaned)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "df['Tense_Cleaned'] = df['Tense'].apply(preprocess_text)\n",
    "    \n",
    "# Check sample\n",
    "print(\"Sample Cleaned Text:\")\n",
    "print(df[['Tense', 'Tense_Cleaned']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"outputs/cleaned_dataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values in 'Tense_Cleaned': 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in 'Tense_Cleaned' column\n",
    "null_count = df['Tense_Cleaned'].isnull().sum()\n",
    "print(f\"Total null values in 'Tense_Cleaned': {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize Texts for Traditional (TF-IDF) and Neural Models (BanglaBERT-Compatible Tokens)\n",
    "\n",
    "- **Objective**: Tokenize cleaned text for traditional models (TF-IDF vectors) and neural models (BanglaBERT tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bnlp import NLTKTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BNLP tokenizer\n",
    "bnlp_tokenizer = NLTKTokenizer()\n",
    "\n",
    "# TF-IDF Tokenization    \n",
    "def bnlp_tokenize(text):\n",
    "    return [token.strip().lower() for token in bnlp_tokenizer.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text representation using tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=bnlp_tokenize, max_features=5000)  \n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Tense_Cleaned'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample TF-IDF Tokens: ['জিনিসপত্রের', 'অতিরিক্ত', 'দাম', 'বৃদ্ধির', 'জন্য', 'এই', 'শহরে', 'জীবন', 'ধারণ', 'করা']\n"
     ]
    }
   ],
   "source": [
    "# Validate tokenization\n",
    "print(\"Sample TF-IDF Tokens:\", bnlp_tokenizer.word_tokenize(df['Tense_Cleaned'].iloc[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF matrix (sparse format)\n",
    "np.savez(\"text_representation/tfidf_matrix.npz\", \n",
    "         data=tfidf_matrix.data, \n",
    "         indices=tfidf_matrix.indices,\n",
    "         indptr=tfidf_matrix.indptr, \n",
    "         shape=tfidf_matrix.shape\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BanglaBERT Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "def tokenize_for_bert(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 8/8 [00:01<00:00,  5.78it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Tokenize in batches\n",
    "batch_size = 1000\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), desc=\"Tokenizing\"):\n",
    "    batch_texts = df['Tense_Cleaned'].values[i:i+batch_size].tolist()\n",
    "\n",
    "    # Tokenize with padding, truncation, and return tensors as NumPy-compatible\n",
    "    batch_tokens = tokenizer(\n",
    "        batch_texts,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='np'  # Ensures NumPy format\n",
    "    )\n",
    "\n",
    "    input_ids.append(batch_tokens['input_ids'])\n",
    "    attention_masks.append(batch_tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack and save\n",
    "bert_input_ids = np.vstack(input_ids)\n",
    "bert_attention_masks = np.vstack(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BERT Tokens: ['[CLS]', 'জিনিস', '##পত', '##রে', '##র', 'অতি', '##রিক', '##ত', 'দাম', 'বদ']\n"
     ]
    }
   ],
   "source": [
    "# Validate tokenization\n",
    "print(\"Sample BERT Tokens:\", tokenizer.convert_ids_to_tokens(bert_input_ids[0][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both input_ids and attention_masks\n",
    "np.save(\"text_representation/bert_input_ids.npy\", bert_input_ids)\n",
    "np.save(\"text_representation/bert_attention_masks.npy\", bert_attention_masks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Dataset into Training, Validation, and Test Sets (Stratified)\n",
    "\n",
    "- **Objective**: Split the dataset into training (80%), validation (10%), and test (10%) sets, preserving class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "    \n",
    "# Stratified split\n",
    "X = df['Tense_Cleaned']\n",
    "y = df['Label']\n",
    "\n",
    "# First split: 90% temp, 10% test\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Second split: ~80% train, ~10% val (because 0.1111 * 0.9 ≈ 0.1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.1111, stratify=y_temp, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Distribution:\n",
      " Label\n",
      "0    47.359922\n",
      "2    29.081221\n",
      "1    23.558857\n",
      "Name: proportion, dtype: float64\n",
      "Validation Set Distribution:\n",
      " Label\n",
      "0    47.354839\n",
      "2    29.032258\n",
      "1    23.612903\n",
      "Name: proportion, dtype: float64\n",
      "Test Set Distribution:\n",
      " Label\n",
      "0    47.354839\n",
      "2    29.032258\n",
      "1    23.612903\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Verify distributions\n",
    "print(\"Training Set Distribution:\\n\", y_train.value_counts(normalize=True) * 100)\n",
    "print(\"Validation Set Distribution:\\n\", y_val.value_counts(normalize=True) * 100)\n",
    "print(\"Test Set Distribution:\\n\", y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split TF-IDF and BERT tokens accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the index of split datasets\n",
    "train_idx, val_idx, test_idx = X_train.index, X_val.index, X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split tf-idf tokens\n",
    "tfidf_train = tfidf_matrix[train_idx]\n",
    "tfidf_val = tfidf_matrix[val_idx]\n",
    "tfidf_test = tfidf_matrix[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split bert tokens\n",
    "bert_train_ids = bert_input_ids[train_idx]\n",
    "bert_val_ids = bert_input_ids[val_idx]\n",
    "bert_test_ids = bert_input_ids[test_idx]\n",
    "\n",
    "bert_train_masks = bert_attention_masks[train_idx]\n",
    "bert_val_masks = bert_attention_masks[val_idx]\n",
    "bert_test_masks = bert_attention_masks[test_idx]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Split indices\n",
    "split_indices = pd.DataFrame({\n",
    "    'Index': list(train_idx) + list(val_idx) + list(test_idx),\n",
    "    'Split': ['Train']*len(train_idx) + ['Val']*len(val_idx) + ['Test']*len(test_idx)\n",
    "})\n",
    "\n",
    "split_indices.to_csv(\"text_representation/split_indices.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save Preprocessed Data\n",
    "\n",
    "- **Objective**: Save preprocessed datasets in formats suitable for traditional and neural models, ensuring compatibility with Phase 3 (model training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Sparse TF-IDF Matrices\n",
    "import scipy.sparse as sp\n",
    "\n",
    "sp.save_npz(\"text_representation/tfidf_train.npz\", tfidf_train)\n",
    "sp.save_npz(\"text_representation/tfidf_val.npz\", tfidf_val)\n",
    "sp.save_npz(\"text_representation/tfidf_test.npz\", tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save labels\n",
    "pd.DataFrame({'Label': y_train}).to_csv(\"text_representation/labels_train.csv\", encoding='utf-8', index=False)\n",
    "pd.DataFrame({'Label': y_val}).to_csv(\"text_representation/labels_val.csv\", encoding='utf-8', index=False)\n",
    "pd.DataFrame({'Label': y_test}).to_csv(\"text_representation/labels_test.csv\", encoding='utf-8', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned text splits\n",
    "pd.DataFrame({'Tense_Cleaned': X_train}).to_csv(\"text_representation/text_train.csv\", encoding='utf-8', index=False)\n",
    "pd.DataFrame({'Tense_Cleaned': X_val}).to_csv(\"text_representation/text_val.csv\", encoding='utf-8', index=False)\n",
    "pd.DataFrame({'Tense_Cleaned': X_test}).to_csv(\"text_representation/text_test.csv\", encoding='utf-8', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Train Shape: (6193, 5000)\n",
      "Labels Train Shape: (6193, 1)\n",
      "BERT Train IDs Shape: (7743, 128)\n"
     ]
    }
   ],
   "source": [
    "# Verify saved files\n",
    "print(\"TF-IDF Train Shape:\", sp.load_npz(\"text_representation/tfidf_train.npz\").shape)\n",
    "print(\"Labels Train Shape:\", pd.read_csv(\"text_representation/labels_train.csv\").shape)\n",
    "print(\"BERT Train IDs Shape:\", np.load(\"text_representation/bert_input_ids.npy\").shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update README with improved formatting\n",
    "with open(\"text_representation/preprocessed_data_README.md\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(\n",
    "        \"# Preprocessed Bangla Sentiment Dataset\\n\\n\"\n",
    "        \"This folder contains all the necessary files for training and evaluating sentiment classification models using Bangla text data.\\n\\n\"\n",
    "        \"## Contents\\n\"\n",
    "        \"- **`cleaned_dataset.csv`**: Original dataset with an additional cleaned text column (`Tense_Cleaned`).\\n\"\n",
    "        \"- **`tfidf_matrix.npz`**: Full sparse TF-IDF matrix for all samples.\\n\"\n",
    "        \"- **`tfidf_train.npz`**, **`tfidf_val.npz`**, **`tfidf_test.npz`**: Sparse TF-IDF matrices for the training, validation, and test sets respectively.\\n\"\n",
    "        \"- **`labels_train.csv`**, **`labels_val.csv`**, **`labels_test.csv`**: Sentiment labels corresponding to each data split.\\n\"\n",
    "        \"- **`bert_input_ids.npy`**: Tokenized input IDs generated using the `BanglaBERT` tokenizer.\\n\"\n",
    "        \"- **`bert_attention_masks.npy`** *(optional if generated)*: Attention masks for BERT inputs.\\n\"\n",
    "        \"- **`text_train.csv`**, **`text_val.csv`**, **`text_test.csv`**: Cleaned Bangla text for each split.\\n\"\n",
    "        \"- **`split_indices.csv`**: Index mapping of samples to their respective dataset split (Train/Val/Test).\\n\\n\"\n",
    "        \"## Notes\\n\"\n",
    "        \"- Labels are encoded as: `0 = Negative`, `1 = Positive`, `2 = Neutral`\\n\"\n",
    "        \"- All text has been preprocessed using BNLP and regex cleaning techniques.\\n\"\n",
    "        \"- BERT tokens are padded to a maximum length of 128.\\n\"\n",
    "        \"- TF-IDF features are saved in SciPy's `.npz` sparse format for efficient loading.\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
