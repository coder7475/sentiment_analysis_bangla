{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Preprocessing and Dataset Preparation\n",
    "\n",
    "**Objective**: Prepare the Bangla Sentiment Dataset (columns: Tense, Label) for model training, ensuring compatibility with traditional (TF-IDF-based) and neural (BanglaBERT) sentiment classification models while preserving class imbalance characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Clean Text (Remove Noise, Normalize Bangla Script)\n",
    "\n",
    "- **Objective**: Clean the `Tense` column to remove noise (e.g., special characters, URLs) and normalize Bangla text for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punkt not found. downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/fahad/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bnlp import CleanText\n",
    "    \n",
    "# Initialize BNLP cleaner\n",
    "clean_text = CleanText(\n",
    "    fix_unicode=True,\n",
    "    unicode_norm=True,\n",
    "    unicode_norm_form=\"NFKC\",\n",
    "    remove_url=False,\n",
    "    remove_email=False,\n",
    "    remove_emoji=False,\n",
    "    remove_number=False,\n",
    "    remove_digits=False,\n",
    "    remove_punct=False,\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"<DIGIT>\",\n",
    "    replace_with_punct = \"<PUNC>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tense</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tense  Label\n",
       "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...      0\n",
       "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...      1\n",
       "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset from phase 1\n",
    "dataset_path = \"data-source/cleaned_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path, encoding=\"utf-8\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Cleaned Text:\n",
      "                                               Tense  \\\n",
      "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...   \n",
      "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...   \n",
      "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...   \n",
      "3          ওনার মতো ব্যর্থ মন্ত্রীর পদত্যাগ করা উচিত   \n",
      "4                 আল্লাহ তোদের বিচার করবে অপেক্ষা কর   \n",
      "\n",
      "                                       Tense_Cleaned  \n",
      "0  জিনিসপত্রের অতিরিক্ত দাম বৃদ্ধির জন্য এই শহরে ...  \n",
      "1  সঠিক ভাবে তদারকি করলে এই সমস্যা থেকে পরিত্রান ...  \n",
      "2  দেশের টাকা যখন বিদেশে চোলে যাচ্ছে তখন দেশের সর...  \n",
      "3          ওনার মতো ব্যর্থ মন্ত্রীর পদত্যাগ করা উচিত  \n",
      "4                 আল্লাহ তোদের বিচার করবে অপেক্ষা কর  \n"
     ]
    }
   ],
   "source": [
    "# Clean text using BNLP\n",
    "def preprocess_text(text):\n",
    "    # BNLP cleaning\n",
    "    cleaned = clean_text(text)\n",
    "    # Additional regex for URLs/hashtags (if BNLP misses any)\n",
    "    cleaned = re.sub(r'http\\S+|#\\S+', '', cleaned)\n",
    "    # Remove extra spaces\n",
    "    cleaned = ' '.join(cleaned.split())\n",
    "    return cleaned\n",
    "\n",
    "df['Tense_Cleaned'] = df['Tense'].apply(preprocess_text)\n",
    "    \n",
    "# Check sample\n",
    "print(\"Sample Cleaned Text:\")\n",
    "print(df[['Tense', 'Tense_Cleaned']].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"outputs/cleaned_dataset.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total null values in 'Tense_Cleaned': 0\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in 'Tense_Cleaned' column\n",
    "null_count = df['Tense_Cleaned'].isnull().sum()\n",
    "print(f\"Total null values in 'Tense_Cleaned': {null_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Tokenize Texts for Traditional (TF-IDF) and Neural Models (BanglaBERT-Compatible Tokens)\n",
    "\n",
    "- **Objective**: Tokenize cleaned text for traditional models (TF-IDF vectors) and neural models (BanglaBERT tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from bnlp import NLTKTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BNLP tokenizer\n",
    "bnlp_tokenizer = NLTKTokenizer()\n",
    "\n",
    "# TF-IDF Tokenization    \n",
    "def bnlp_tokenize(text):\n",
    "    return [token.strip().lower() for token in bnlp_tokenizer.word_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text representation using tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=bnlp_tokenize, max_features=5000)  \n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Tense_Cleaned'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample TF-IDF Tokens: ['সঠিক', 'ভাবে', 'তদারকি', 'করলে', 'এই', 'সমস্যা', 'থেকে', 'পরিত্রান', 'পওয়া', 'সম্ভব']\n"
     ]
    }
   ],
   "source": [
    "# Validate tokenization\n",
    "print(\"Sample TF-IDF Tokens:\", bnlp_tokenizer.word_tokenize(df['Tense_Cleaned'].iloc[1])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF-IDF matrix (sparse format)\n",
    "np.savez(\"text_representation/tfidf_matrix.npz\", \n",
    "         data=tfidf_matrix.data, \n",
    "         indices=tfidf_matrix.indices,\n",
    "         indptr=tfidf_matrix.indptr, \n",
    "         shape=tfidf_matrix.shape\n",
    "        )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6b3a5e80f54694ae1e619df8227f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242464cead1343999eca851edd9f457a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/2.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# BanglaBERT Tokenization\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n",
    "\n",
    "def tokenize_for_bert(text):\n",
    "    return tokenizer(text, padding='max_length', truncation=True, max_length=128, return_tensors='np')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize in batches to manage memory\n",
    "batch_size = 1000\n",
    "bert_tokens = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    batch = df['Tense_Cleaned'][i:i+batch_size].tolist()\n",
    "    tokens = tokenizer(batch, padding='max_length', truncation=True, max_length=128, return_tensors='np')\n",
    "    bert_tokens.append(tokens['input_ids'])\n",
    "    bert_input_ids = np.vstack(bert_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample BERT Tokens: ['[CLS]', 'জিনিস', '##পত', '##রে', '##র', 'অতি', '##রিক', '##ত', 'দাম', 'বদ']\n"
     ]
    }
   ],
   "source": [
    "# Validate tokenization\n",
    "print(\"Sample BERT Tokens:\", tokenizer.convert_ids_to_tokens(bert_input_ids[0][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERT tokens\n",
    "np.save(\"text_representation/bert_input_ids.npy\", bert_input_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split Dataset into Training, Validation, and Test Sets (Stratified)\n",
    "\n",
    "- **Objective**: Split the dataset into training (80%), validation (10%), and test (10%) sets, preserving class imbalance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
