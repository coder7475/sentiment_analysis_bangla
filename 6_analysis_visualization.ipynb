{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6: Analysis and Visualization\n",
    "\n",
    "**Objective**: Interpret results from Phases 3â€“5 to identify class imbalance effects, compare mitigation techniques (SMOTE, Random Undersampling, NearMiss, Weighted Loss), and visualize findings to highlight improvements in 3-class (Negative, Neutral, Positive) sentiment classification on the Bangla Sentiment Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Analyze Imbalance Effects\n",
    "\n",
    "- **Objective**: Identify performance gaps in baseline models, focusing on minority class (Positive) recall and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Ensure 'analysis' folder exists\n",
    "os.makedirs(\"analysis\", exist_ok=True)\n",
    "\n",
    "# Load results\n",
    "try:\n",
    "    results_df = pd.read_csv(\"evaluation/comparative_results.csv\")\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"CSV file not found. Make sure 'evaluation/comparative_results.csv' exists.\")\n",
    "    exit(1)\n",
    "\n",
    "# Filter by Type\n",
    "baseline_df = results_df[results_df['Type'] == 'baseline']\n",
    "mitigated_df = results_df[results_df['Type'] == 'mitigated']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:26:01,924 - INFO - Baseline Positive class summary:\n",
      "       F1_Positive   ROC_AUC\n",
      "count     4.000000  4.000000\n",
      "mean      0.508565  0.744059\n",
      "std       0.022895  0.007840\n",
      "min       0.478571  0.734816\n",
      "25%       0.497341  0.741040\n",
      "50%       0.512668  0.743732\n",
      "75%       0.523892  0.746751\n",
      "max       0.530351  0.753958\n"
     ]
    }
   ],
   "source": [
    "# Summarize Positive class performance for baseline\n",
    "positive_summary = baseline_df[['Model', 'F1_Positive', 'ROC_AUC']].describe()\n",
    "logging.info(\"Baseline Positive class summary:\\n\" + str(positive_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:26:55,333 - INFO - Mitigated Positive class F1 (mean):\n",
      "Mitigation                       tuned\n",
      "Model                                 \n",
      "LogisticRegression_nearmiss      0.500\n",
      "LogisticRegression_smote         0.504\n",
      "LogisticRegression_undersampled  0.524\n",
      "LogisticRegression_weighted      0.480\n",
      "NaiveBayes_nearmiss              0.528\n",
      "NaiveBayes_smote                 0.475\n",
      "NaiveBayes_undersampled          0.523\n",
      "NaiveBayes_weighted              0.475\n",
      "RandomForest_nearmiss            0.513\n",
      "RandomForest_smote               0.484\n",
      "RandomForest_undersampled        0.544\n",
      "RandomForest_weighted            0.484\n",
      "SVM_nearmiss                     0.531\n",
      "SVM_smote                        0.521\n",
      "SVM_undersampled                 0.540\n",
      "SVM_weighted                     0.519\n"
     ]
    }
   ],
   "source": [
    "# Compute standard deviation and mean for additional insight\n",
    "baseline_f1_mean = baseline_df['F1_Positive'].mean()\n",
    "baseline_f1_std = baseline_df['F1_Positive'].std()\n",
    "\n",
    "# Compare with mitigated models\n",
    "mitigated_positive = (\n",
    "    mitigated_df.groupby(['Model', 'Mitigation'])['F1_Positive']\n",
    "    .mean()\n",
    "    .unstack()\n",
    "    .round(3)\n",
    ")\n",
    "logging.info(\"Mitigated Positive class F1 (mean):\\n\" + str(mitigated_positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 18:27:29,326 - INFO - Imbalance analysis saved: analysis/imbalance_analysis.txt\n"
     ]
    }
   ],
   "source": [
    "# Write analysis\n",
    "analysis_path = \"analysis/imbalance_analysis.txt\"\n",
    "with open(analysis_path, \"w\", encoding='utf-8') as f:\n",
    "    f.write(\"ðŸ“Š Imbalance Effects Analysis\\n\")\n",
    "    f.write(\"=================================\\n\")\n",
    "    f.write(\"Baseline models show low Positive class F1:\\n\")\n",
    "    f.write(\"  - Mean F1_Positive: {:.3f}\\n\".format(baseline_f1_mean))\n",
    "    f.write(\"  - Std Dev F1_Positive: {:.3f}\\n\".format(baseline_f1_std))\n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"Mitigated models improve Positive F1, especially with techniques like SMOTE and Weighted Loss:\\n\\n\")\n",
    "    f.write(mitigated_positive.to_string())\n",
    "    f.write(\"\\n\\nRecommendations:\\n\")\n",
    "    f.write(\"- Use SMOTE or Weighted Loss for imbalanced classification tasks.\\n\")\n",
    "    f.write(\"- Consider additional metrics like precision/recall per class for deeper insight.\\n\")\n",
    "\n",
    "logging.info(f\"Imbalance analysis saved: {analysis_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
