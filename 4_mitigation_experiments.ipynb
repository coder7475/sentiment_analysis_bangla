{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Phase 4: Mitigation Experiments (Month 3–Early Month 4)\n",
    "\n",
    "**Objective**: Improve performance on 3-class (Negative, Neutral, Positive) sentiment classification on the Bangla Sentiment Dataset by applying imbalanced learning strategies, including data-level (SMOTE, Random Undersampling, NearMiss) and algorithm-level (Weighted Loss) methods. Train and tune Logistic Regression, SVM, Naive Bayes, and Random Forest, with comprehensive evaluations and visualizations (confusion matrices, ROC-AUC curves, precision-recall curves, F1 comparisons) to assess mitigation effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:54:49,806 - INFO - TF-IDF matrices loaded successfully\n",
      "2025-06-23 17:54:49,821 - INFO - Labels loaded successfully\n",
      "2025-06-23 17:54:49,823 - INFO - Data shapes validated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Train Shape: (6193, 5000)\n",
      "Labels Train Shape: (6193,)\n",
      "Label Distribution (Train):\n",
      " 0    47.359922\n",
      "2    29.081221\n",
      "1    23.558857\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"text_representation/\"\n",
    "files = {\n",
    "    'tfidf_train': f\"{data_dir}tfidf_train.npz\",\n",
    "    'tfidf_val': f\"{data_dir}tfidf_val.npz\",\n",
    "    'tfidf_test': f\"{data_dir}tfidf_test.npz\",\n",
    "    'labels_train': f\"{data_dir}labels_train.csv\",\n",
    "    'labels_val': f\"{data_dir}labels_val.csv\",\n",
    "    'labels_test': f\"{data_dir}labels_test.csv\"\n",
    "}\n",
    "\n",
    "# Check file existence\n",
    "for name, path in files.items():\n",
    "    if not os.path.exists(path):\n",
    "        logging.error(f\"Missing file: {path}\")\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "# Load TF-IDF matrices\n",
    "tfidf_train = sp.load_npz(files['tfidf_train'])\n",
    "tfidf_val = sp.load_npz(files['tfidf_val'])\n",
    "tfidf_test = sp.load_npz(files['tfidf_test'])\n",
    "logging.info(\"TF-IDF matrices loaded successfully\")\n",
    "\n",
    "# Load labels\n",
    "y_train = pd.read_csv(files['labels_train'], encoding='utf-8')['Label'].values\n",
    "y_val = pd.read_csv(files['labels_val'], encoding='utf-8')['Label'].values\n",
    "y_test = pd.read_csv(files['labels_test'], encoding='utf-8')['Label'].values\n",
    "logging.info(\"Labels loaded successfully\")\n",
    "\n",
    "# Validate shapes\n",
    "assert tfidf_train.shape[0] == len(y_train), \"Train data mismatch\"\n",
    "assert tfidf_val.shape[0] == len(y_val), \"Validation data mismatch\"\n",
    "assert tfidf_test.shape[0] == len(y_test), \"Test data mismatch\"\n",
    "logging.info(\"Data shapes validated\")\n",
    "\n",
    "# Print shapes and distribution\n",
    "print(\"TF-IDF Train Shape:\", tfidf_train.shape)\n",
    "print(\"Labels Train Shape:\", y_train.shape)\n",
    "print(\"Label Distribution (Train):\\n\", pd.Series(y_train).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Apply Data-Level Mitigation Techniques\n",
    "\n",
    "- **Objective**: Generate mitigated datasets using SMOTE (oversampling), Random Undersampling, and NearMiss (undersampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.7.0)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Collecting scikit-learn<2,>=1.3.2 (from imbalanced-learn)\n",
      "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m818.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Installing collected packages: scikit-learn, sklearn-compat, imbalanced-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.7.0\n",
      "    Uninstalling scikit-learn-1.7.0:\n",
      "      Successfully uninstalled scikit-learn-1.7.0\n",
      "Successfully installed imbalanced-learn-0.13.0 scikit-learn-1.6.1 sklearn-compat-0.1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 17:59:26,166 - INFO - SMOTE dataset saved\n",
      "2025-06-23 17:59:26,227 - INFO - Undersampled dataset saved\n",
      "2025-06-23 17:59:26,529 - INFO - NearMiss dataset saved\n",
      "2025-06-23 17:59:26,540 - INFO - Class weights computed\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "tfidf_train_smote, y_train_smote = smote.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_smote.npz\", tfidf_train_smote)\n",
    "pd.DataFrame({'Label': y_train_smote}).to_csv(\"mitigated_datasets/labels_train_smote.csv\", index=False)\n",
    "logging.info(\"SMOTE dataset saved\")\n",
    "\n",
    "# Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "tfidf_train_under, y_train_under = undersampler.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_undersampled.npz\", tfidf_train_under)\n",
    "pd.DataFrame({'Label': y_train_under}).to_csv(\"mitigated_datasets/labels_train_undersampled.csv\", index=False)\n",
    "logging.info(\"Undersampled dataset saved\")\n",
    "\n",
    "# NearMiss (version 1)\n",
    "nearmiss = NearMiss(version=1, n_neighbors=3)\n",
    "tfidf_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_nearmiss.npz\", tfidf_train_nearmiss)\n",
    "pd.DataFrame({'Label': y_train_nearmiss}).to_csv(\"mitigated_datasets/labels_train_nearmiss.csv\", index=False)\n",
    "logging.info(\"NearMiss dataset saved\")\n",
    "\n",
    "# Class weights for algorithm-level mitigation\n",
    "class_weights = {i: 1.0 / pd.Series(y_train).value_counts()[i] for i in range(3)}\n",
    "total = sum(class_weights.values())\n",
    "class_weights = {k: v / total * 3 for k, v in class_weights.items()}\n",
    "logging.info(\"Class weights computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Retrain Logistic Regression, SVM, Naive Bayes, and Random Forest\n",
    "\n",
    "- **Objective**: Train models on mitigated datasets (SMOTE, Random Undersampling, NearMiss) and apply weighted loss to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:01:04,985 - INFO - Output directory created: models/mitigated_models\n",
      "Training Datasets:   0%|          | 0/4 [00:00<?, ?it/s]/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "2025-06-23 18:01:06,661 - INFO - LogisticRegression (smote) trained and saved\n",
      "2025-06-23 18:02:38,660 - INFO - SVM (smote) trained and saved\n",
      "2025-06-23 18:02:38,662 - ERROR - Error training NaiveBayes (smote): Invalid parameter 'class_weight' for estimator MultinomialNB(). Valid parameters are: ['alpha', 'class_prior', 'fit_prior', 'force_alpha'].\n",
      "2025-06-23 18:02:55,345 - INFO - RandomForest (smote) trained and saved\n",
      "Training Datasets:  25%|██▌       | 1/4 [01:50<05:31, 110.35s/it]/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "2025-06-23 18:02:55,725 - INFO - LogisticRegression (undersampled) trained and saved\n",
      "2025-06-23 18:03:19,694 - INFO - SVM (undersampled) trained and saved\n",
      "2025-06-23 18:03:19,696 - ERROR - Error training NaiveBayes (undersampled): Invalid parameter 'class_weight' for estimator MultinomialNB(). Valid parameters are: ['alpha', 'class_prior', 'fit_prior', 'force_alpha'].\n",
      "2025-06-23 18:03:30,147 - INFO - RandomForest (undersampled) trained and saved\n",
      "Training Datasets:  50%|█████     | 2/4 [02:25<02:11, 65.91s/it] /home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "2025-06-23 18:03:30,800 - INFO - LogisticRegression (nearmiss) trained and saved\n",
      "2025-06-23 18:03:56,570 - INFO - SVM (nearmiss) trained and saved\n",
      "2025-06-23 18:03:56,572 - ERROR - Error training NaiveBayes (nearmiss): Invalid parameter 'class_weight' for estimator MultinomialNB(). Valid parameters are: ['alpha', 'class_prior', 'fit_prior', 'force_alpha'].\n",
      "2025-06-23 18:04:08,885 - INFO - RandomForest (nearmiss) trained and saved\n",
      "Training Datasets:  75%|███████▌  | 3/4 [03:03<00:53, 53.50s/it]/home/fahad/projects/personal/sentiment_analysis_bangla/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "2025-06-23 18:04:12,423 - INFO - LogisticRegression (weighted) trained and saved\n",
      "2025-06-23 18:05:59,523 - INFO - SVM (weighted) trained and saved\n",
      "2025-06-23 18:05:59,529 - ERROR - Error training NaiveBayes (weighted): Invalid parameter 'class_weight' for estimator MultinomialNB(). Valid parameters are: ['alpha', 'class_prior', 'fit_prior', 'force_alpha'].\n",
      "2025-06-23 18:06:21,025 - INFO - RandomForest (weighted) trained and saved\n",
      "Training Datasets: 100%|██████████| 4/4 [05:16<00:00, 79.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"models/mitigated_models\", exist_ok=True)\n",
    "logging.info(\"Output directory created: models/mitigated_models\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, multi_class='multinomial', penalty='l2', random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    'smote': (tfidf_train_smote, y_train_smote),\n",
    "    'undersampled': (tfidf_train_under, y_train_under),\n",
    "    'nearmiss': (tfidf_train_nearmiss, y_train_nearmiss),\n",
    "    'weighted': (tfidf_train_smote, y_train_smote)  # Weighted uses SMOTE for consistency\n",
    "}\n",
    "\n",
    "# Train models\n",
    "for dataset_name, (X_train, y_train) in tqdm(datasets.items(), desc=\"Training Datasets\"):\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if dataset_name == 'weighted' and name in ['LogisticRegression', 'SVM']:\n",
    "                model.set_params(class_weight=class_weights)\n",
    "            else:\n",
    "                model.set_params(class_weight=None)\n",
    "            model.fit(X_train, y_train)\n",
    "            joblib.dump(model, f\"models/mitigated_models/{name}_{dataset_name}.joblib\")\n",
    "            logging.info(f\"{name} ({dataset_name}) trained and saved\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training {name} ({dataset_name}): {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
