{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revised Phase 4: Mitigation Experiments (Month 3â€“Early Month 4)\n",
    "\n",
    "**Objective**: Improve performance on 3-class (Negative, Neutral, Positive) sentiment classification on the Bangla Sentiment Dataset by applying imbalanced learning strategies, including data-level (SMOTE, Random Undersampling, NearMiss) and algorithm-level (Weighted Loss) methods. Train and tune Logistic Regression, SVM, Naive Bayes, and Random Forest, with comprehensive evaluations and visualizations (confusion matrices, ROC-AUC curves, precision-recall curves, F1 comparisons) to assess mitigation effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:29:01,532 - INFO - TF-IDF matrices loaded successfully\n",
      "2025-06-23 18:29:01,554 - INFO - Labels loaded successfully\n",
      "2025-06-23 18:29:01,557 - INFO - Data shapes validated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Train Shape: (6193, 5000)\n",
      "Labels Train Shape: (6193,)\n",
      "Label Distribution (Train):\n",
      " 0    47.359922\n",
      "2    29.081221\n",
      "1    23.558857\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"text_representation/\"\n",
    "files = {\n",
    "    'tfidf_train': f\"{data_dir}tfidf_train.npz\",\n",
    "    'tfidf_val': f\"{data_dir}tfidf_val.npz\",\n",
    "    'tfidf_test': f\"{data_dir}tfidf_test.npz\",\n",
    "    'labels_train': f\"{data_dir}labels_train.csv\",\n",
    "    'labels_val': f\"{data_dir}labels_val.csv\",\n",
    "    'labels_test': f\"{data_dir}labels_test.csv\"\n",
    "}\n",
    "\n",
    "# Check file existence\n",
    "for name, path in files.items():\n",
    "    if not os.path.exists(path):\n",
    "        logging.error(f\"Missing file: {path}\")\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "# Load TF-IDF matrices\n",
    "tfidf_train = sp.load_npz(files['tfidf_train'])\n",
    "tfidf_val = sp.load_npz(files['tfidf_val'])\n",
    "tfidf_test = sp.load_npz(files['tfidf_test'])\n",
    "logging.info(\"TF-IDF matrices loaded successfully\")\n",
    "\n",
    "# Load labels\n",
    "y_train = pd.read_csv(files['labels_train'], encoding='utf-8')['Label'].values\n",
    "y_val = pd.read_csv(files['labels_val'], encoding='utf-8')['Label'].values\n",
    "y_test = pd.read_csv(files['labels_test'], encoding='utf-8')['Label'].values\n",
    "logging.info(\"Labels loaded successfully\")\n",
    "\n",
    "# Validate shapes\n",
    "assert tfidf_train.shape[0] == len(y_train), \"Train data mismatch\"\n",
    "assert tfidf_val.shape[0] == len(y_val), \"Validation data mismatch\"\n",
    "assert tfidf_test.shape[0] == len(y_test), \"Test data mismatch\"\n",
    "logging.info(\"Data shapes validated\")\n",
    "\n",
    "# Print shapes and distribution\n",
    "print(\"TF-IDF Train Shape:\", tfidf_train.shape)\n",
    "print(\"Labels Train Shape:\", y_train.shape)\n",
    "print(\"Label Distribution (Train):\\n\", pd.Series(y_train).value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Apply Data-Level Mitigation Techniques\n",
    "\n",
    "- **Objective**: Generate mitigated datasets using SMOTE (oversampling), Random Undersampling, and NearMiss (undersampling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in ./venv/lib/python3.11/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in ./venv/lib/python3.11/site-packages (from imbalanced-learn) (3.6.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:29:08,542 - INFO - SMOTE dataset saved\n",
      "2025-06-23 18:29:08,615 - INFO - Undersampled dataset saved\n",
      "2025-06-23 18:29:09,094 - INFO - NearMiss dataset saved\n",
      "2025-06-23 18:29:09,099 - INFO - Class weights computed\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "tfidf_train_smote, y_train_smote = smote.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_smote.npz\", tfidf_train_smote)\n",
    "pd.DataFrame({'Label': y_train_smote}).to_csv(\"mitigated_datasets/labels_train_smote.csv\", index=False)\n",
    "logging.info(\"SMOTE dataset saved\")\n",
    "\n",
    "# Random Undersampling\n",
    "undersampler = RandomUnderSampler(random_state=42)\n",
    "tfidf_train_under, y_train_under = undersampler.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_undersampled.npz\", tfidf_train_under)\n",
    "pd.DataFrame({'Label': y_train_under}).to_csv(\"mitigated_datasets/labels_train_undersampled.csv\", index=False)\n",
    "logging.info(\"Undersampled dataset saved\")\n",
    "\n",
    "# NearMiss (version 1)\n",
    "nearmiss = NearMiss(version=1, n_neighbors=3)\n",
    "tfidf_train_nearmiss, y_train_nearmiss = nearmiss.fit_resample(tfidf_train, y_train)\n",
    "sp.save_npz(\"mitigated_datasets/tfidf_train_nearmiss.npz\", tfidf_train_nearmiss)\n",
    "pd.DataFrame({'Label': y_train_nearmiss}).to_csv(\"mitigated_datasets/labels_train_nearmiss.csv\", index=False)\n",
    "logging.info(\"NearMiss dataset saved\")\n",
    "\n",
    "# Class weights for algorithm-level mitigation\n",
    "class_weights = {i: 1.0 / pd.Series(y_train).value_counts()[i] for i in range(3)}\n",
    "total = sum(class_weights.values())\n",
    "class_weights = {k: v / total * 3 for k, v in class_weights.items()}\n",
    "logging.info(\"Class weights computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Retrain Logistic Regression, SVM, Naive Bayes, and Random Forest\n",
    "\n",
    "- **Objective**: Train models on mitigated datasets (SMOTE, Random Undersampling, NearMiss) and apply weighted loss to all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-23 18:29:09,124 - INFO - Output directory created: models/mitigated_models\n",
      "Training Datasets:   0%|          | 0/4 [00:00<?, ?it/s]2025-06-23 18:29:10,436 - INFO - LogisticRegression (smote) trained and saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"models/mitigated_models\", exist_ok=True)\n",
    "logging.info(\"Output directory created: models/mitigated_models\")\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000, penalty='l2', random_state=42),  # Removed deprecated multi_class\n",
    "    'SVM': SVC(probability=True, random_state=42),\n",
    "    'NaiveBayes': MultinomialNB(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Define datasets\n",
    "datasets = {\n",
    "    'smote': (tfidf_train_smote, y_train_smote),\n",
    "    'undersampled': (tfidf_train_under, y_train_under),\n",
    "    'nearmiss': (tfidf_train_nearmiss, y_train_nearmiss),\n",
    "    'weighted': (tfidf_train_smote, y_train_smote)  # Same data, but algorithm-level class_weight is applied\n",
    "}\n",
    "\n",
    "# Supported class_weight models\n",
    "supports_class_weight = {'LogisticRegression', 'SVM', 'RandomForest'}\n",
    "\n",
    "# Train models\n",
    "for dataset_name, (X_train, y_train) in tqdm(datasets.items(), desc=\"Training Datasets\"):\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            # Clone model to avoid contamination\n",
    "            model_copy = model.__class__(**model.get_params())\n",
    "\n",
    "            # Set class_weight only when applicable\n",
    "            if dataset_name == 'weighted' and name in supports_class_weight:\n",
    "                model_copy.set_params(class_weight=class_weights)\n",
    "            elif 'class_weight' in model_copy.get_params():\n",
    "                model_copy.set_params(class_weight=None)\n",
    "\n",
    "            model_copy.fit(X_train, y_train)\n",
    "\n",
    "            model_path = f\"models/mitigated_models/{name}_{dataset_name}.joblib\"\n",
    "            joblib.dump(model_copy, model_path)\n",
    "            logging.info(f\"{name} ({dataset_name}) trained and saved\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error training {name} ({dataset_name}): {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Hyperparameter Tuning\n",
    "\n",
    "- **Objective**: Tune Logistic Regression, SVM, Naive Bayes, and Random Forest on mitigated datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Tune models\u001b[39;00m\n\u001b[32m     31\u001b[39m tuned_results = {}\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, (X_train, y_train) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[43mdatasets\u001b[49m.items(), desc=\u001b[33m\"\u001b[39m\u001b[33mTuning Datasets\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     34\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "        'penalty': ['l2'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'NaiveBayes': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],  # Prevent overfitting\n",
    "    }\n",
    "}\n",
    "# Tune models\n",
    "tuned_results = {}\n",
    "\n",
    "for dataset_name, (X_train, y_train) in tqdm(datasets.items(), desc=\"Tuning Datasets\"):\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            if dataset_name == 'weighted' and name in ['LogisticRegression', 'SVM']:\n",
    "                model.set_params(class_weight=class_weights)\n",
    "            grid = GridSearchCV(\n",
    "                estimator=model,\n",
    "                param_grid=param_grids[name],\n",
    "                scoring='f1_weighted',\n",
    "                cv=5,\n",
    "                n_jobs=-1,\n",
    "                verbose=1\n",
    "            )\n",
    "            grid.fit(X_train, y_train)\n",
    "            tuned_results[f\"{name}_{dataset_name}\"] = {\n",
    "                'best_params': grid.best_params_,\n",
    "                'best_score': grid.best_score_\n",
    "            }\n",
    "            joblib.dump(grid.best_estimator_, f\"models/mitigated_models/{name}_{dataset_name}_tuned.joblib\")\n",
    "            logging.info(f\"{name} ({dataset_name}) tuned and saved: {grid.best_params_}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error tuning {name} ({dataset_name}): {str(e)}\")\n",
    "\n",
    "# Save tuning results\n",
    "pd.DataFrame(tuned_results).to_csv(\"models/mitigated_models/tuned_results.csv\")\n",
    "logging.info(\"Tuning results saved: tuned_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate and Compare\n",
    "\n",
    "- **Objective**: Evaluate mitigated models on test set, compare with Phase 3 baselines, and visualize performance, emphasizing Positive class with precision-recall curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, confusion_matrix, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import logging\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Negative', 'Positive', 'Neutral']\n",
    "\n",
    "# Initialize results\n",
    "f1_scores = {'Model': [], 'Mitigation': [], 'F1_Weighted': [], 'F1_Negative': [], 'F1_Positive': [], 'F1_Neutral': [], 'PR_AUC_Positive': []}\n",
    "lb = LabelBinarizer()\n",
    "y_test_bin = lb.fit_transform(y_test)\n",
    "\n",
    "# Evaluate models\n",
    "for name in ['LogisticRegression', 'SVM', 'NaiveBayes', 'RandomForest']:\n",
    "    for dataset_name in ['smote', 'undersampled', 'nearmiss', 'weighted']:\n",
    "        try:\n",
    "            model = joblib.load(f\"models/mitigated_models/{name}_{dataset_name}_tuned.joblib\")\n",
    "            logging.info(f\"Loaded model: {name}_{dataset_name}_tuned.joblib\")\n",
    "            y_pred = model.predict(tfidf_test)\n",
    "            y_pred_proba = model.predict_proba(tfidf_test)\n",
    "\n",
    "            # Compute metrics\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "            f1_per_class = precision_recall_fscore_support(y_test, y_pred)[2]\n",
    "            precision_positive, recall_positive, _ = precision_recall_curve(y_test_bin[:, 1], y_pred_proba[:, 1])\n",
    "            pr_auc_positive = auc(recall_positive, precision_positive)\n",
    "\n",
    "            # Store results\n",
    "            f1_scores['Model'].append(name)\n",
    "            f1_scores['Mitigation'].append(dataset_name)\n",
    "            f1_scores['F1_Weighted'].append(f1)\n",
    "            f1_scores['F1_Negative'].append(f1_per_class[0])\n",
    "            f1_scores['F1_Positive'].append(f1_per_class[1])\n",
    "            f1_scores['F1_Neutral'].append(f1_per_class[2])\n",
    "            f1_scores['PR_AUC_Positive'].append(pr_auc_positive)\n",
    "\n",
    "            # Confusion Matrix\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "            plt.title(f\"{name} ({dataset_name}) Confusion Matrix\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"True\")\n",
    "            plt.savefig(f\"models/mitigated_models/{name}_{dataset_name}_cm.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            logging.info(f\"Saved confusion matrix: {name}_{dataset_name}_cm.png\")\n",
    "\n",
    "            # ROC-AUC Curve\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            for i in range(len(class_names)):\n",
    "                fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "                auc_score = roc_auc_score(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "                plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {auc_score:.2f})\")\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label='Chance')\n",
    "            plt.xlabel('False Positive Rate')\n",
    "            plt.ylabel('True Positive Rate')\n",
    "            plt.title(f\"{name} ({dataset_name}) ROC-AUC Curves\")\n",
    "            plt.legend(loc='lower right')\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"models/mitigated_models/{name}_{dataset_name}_roc_auc.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            logging.info(f\"Saved ROC-AUC plot: {name}_{dataset_name}_roc_auc.png\")\n",
    "\n",
    "            # Precision-Recall Curve for Positive Class\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.plot(recall_positive, precision_positive, label=f'Positive (PR-AUC = {pr_auc_positive:.2f})')\n",
    "            plt.xlabel('Recall')\n",
    "            plt.ylabel('Precision')\n",
    "            plt.title(f\"{name} ({dataset_name}) Precision-Recall Curve (Positive Class)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.savefig(f\"models/mitigated_models/{name}_{dataset_name}_pr_curve.png\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            logging.info(f\"Saved PR curve: {name}_{dataset_name}_pr_curve.png\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error evaluating {name} ({dataset_name}): {str(e)}\")\n",
    "\n",
    "# Plot comparisons\n",
    "f1_df = pd.DataFrame(f1_scores)\n",
    "sns.barplot(x='Mitigation', y='F1_Weighted', hue='Model', data=f1_df)\n",
    "plt.title(\"Weighted F1 Comparison Across Mitigation Strategies\")\n",
    "plt.savefig(\"models/mitigated_models/f1_comparison_weighted.png\")\n",
    "plt.close()\n",
    "logging.info(\"Saved F1 comparison plot\")\n",
    "\n",
    "sns.barplot(x='Mitigation', y='PR_AUC_Positive', hue='Model', data=f1_df)\n",
    "plt.title(\"Precision-Recall AUC (Positive Class) Comparison\")\n",
    "plt.savefig(\"models/mitigated_models/pr_auc_positive_comparison.png\")\n",
    "plt.close()\n",
    "logging.info(\"Saved PR-AUC comparison plot\")\n",
    "\n",
    "# Save results\n",
    "f1_df.to_csv(\"models/mitigated_models/test_results.csv\")\n",
    "logging.info(\"Test results saved: test_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update README\n",
    "with open(\"models/mitigated_models/README.txt\", \"w\", encoding='utf-8') as f:\n",
    "    f.write(\"Mitigated Model Outputs:\\n\"\n",
    "            \"- mitigated_datasets/*.npz, *.csv: SMOTE, Undersampled, NearMiss datasets\\n\"\n",
    "            \"- models/mitigated_models/*_*.joblib: Trained models\\n\"\n",
    "            \"- models/mitigated_models/*_tuned.joblib: Tuned models\\n\"\n",
    "            \"- tuned_results.csv: Tuning results\\n\"\n",
    "            \"- test_results.csv: Test set metrics\\n\"\n",
    "            \"- *_cm.png: Confusion matrices\\n\"\n",
    "            \"- *_roc_auc.png: ROC-AUC curves\\n\"\n",
    "            \"- *_pr_curve.png: Precision-recall curves\\n\"\n",
    "            \"- f1_comparison_*.png: F1 comparison plots\\n\"\n",
    "            \"- pr_auc_positive_comparison.png: PR-AUC comparison\")\n",
    "logging.info(\"README updated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
